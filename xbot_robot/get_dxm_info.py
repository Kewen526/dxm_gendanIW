# ä½¿ç”¨æé†’:
# 1. xbotåŒ…æä¾›è½¯ä»¶è‡ªåŠ¨åŒ–ã€æ•°æ®è¡¨æ ¼ã€Excelã€æ—¥å¿—ã€AIç­‰åŠŸèƒ½
# 2. packageåŒ…æä¾›è®¿é—®å½“å‰åº”ç”¨æ•°æ®çš„åŠŸèƒ½ï¼Œå¦‚è·å–å…ƒç´ ã€è®¿é—®å…¨å±€å˜é‡ã€è·å–èµ„æºæ–‡ä»¶ç­‰åŠŸèƒ½
# 3. å½“æ­¤æ¨¡å—ä½œä¸ºæµç¨‹ç‹¬ç«‹è¿è¡Œæ—¶æ‰§è¡Œmainå‡½æ•°
# 4. å¯è§†åŒ–æµç¨‹ä¸­å¯ä»¥é€šè¿‡"è°ƒç”¨æ¨¡å—"çš„æŒ‡ä»¤ä½¿ç”¨æ­¤æ¨¡å—

import xbot
from xbot import print, sleep
from .import package
from .package import variables as glv

def main(args):
    pass
import json
import requests
from datetime import datetime, timedelta
from typing import List, Dict
import time
import random
import os


class DianxiaomiScraper:
    def __init__(self, cookie_json_path: str, debug: bool = True):
        self.cookie_json_path = cookie_json_path
        self.debug = debug
        self.session = requests.Session()
        self.base_url = "https://www.dianxiaomi.com/api/package/list.json"
        self._load_cookies()
        
    def _debug_print(self, message: str):
        if self.debug:
            print(message)
        
    def _load_cookies(self):
        try:
            print(f"[INFO] æ­£åœ¨è¯»å–cookieæ–‡ä»¶: {self.cookie_json_path}")
            
            with open(self.cookie_json_path, 'r', encoding='utf-8') as f:
                cookie_data = json.load(f)
            
            self._debug_print(f"[DEBUG] Cookieæ–‡ä»¶é”®: {list(cookie_data.keys())}")
            
            cookies_list = cookie_data.get('cookies', [])
            
            if not cookies_list:
                print("âš ï¸ è­¦å‘Š: cookieæ–‡ä»¶ä¸­æ²¡æœ‰cookieså­—æ®µæˆ–ä¸ºç©º")
            
            print(f"[INFO] æ‰¾åˆ° {len(cookies_list)} ä¸ªcookies")
            
            for i, cookie in enumerate(cookies_list):
                cookie_name = cookie.get('name', 'unknown')
                cookie_domain = cookie.get('domain', '')
                cookie_value = cookie.get('value', '')
                
                self._debug_print(f"[DEBUG] Cookie {i+1}: {cookie_name} (domain: {cookie_domain}, valueé•¿åº¦: {len(cookie_value)})")
                
                self.session.cookies.set(
                    name=cookie.get('name'),
                    value=cookie.get('value'),
                    domain=cookie.get('domain', ''),
                    path=cookie.get('path', '/')
                )
            
            print(f"âœ“ æˆåŠŸåŠ è½½ {len(cookies_list)} ä¸ªcookies")
            self._debug_print(f"[DEBUG] Sessionä¸­çš„cookiesæ•°é‡: {len(self.session.cookies)}")
            
        except FileNotFoundError:
            print(f"âœ— é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {self.cookie_json_path}")
            raise
        except json.JSONDecodeError as e:
            print(f"âœ— é”™è¯¯: JSONæ–‡ä»¶æ ¼å¼é”™è¯¯")
            self._debug_print(f"[DEBUG] é”™è¯¯è¯¦æƒ…: {str(e)}")
            raise
        except Exception as e:
            print(f"âœ— åŠ è½½cookieæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
            raise
    
    def _get_headers(self) -> Dict[str, str]:
        return {
            'authority': 'www.dianxiaomi.com',
            'accept': 'application/json, text/plain, */*',
            'accept-language': 'zh-CN,zh;q=0.9',
            'bx-v': '2.5.11',
            'content-type': 'application/x-www-form-urlencoded',
            'origin': 'https://www.dianxiaomi.com',
            'referer': 'https://www.dianxiaomi.com/web/order/all?go=m1-1',
            'sec-ch-ua': '"Chromium";v="109", "Not_A Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"',
            'sec-fetch-dest': 'empty',
            'sec-fetch-mode': 'cors',
            'sec-fetch-site': 'same-origin',
            'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
        }
    
    def _build_post_data(self, page_no: int = 1, page_size: int = 300) -> Dict[str, str]:
        return {
            'pageNo': str(page_no),
            'pageSize': str(page_size),
            'shopId': '-1',
            'state': '',
            'platform': '',
            'isSearch': '1',
            'searchType': 'orderId',
            'authId': '-1',
            'startTime': '',
            'endTime': '',
            'country': '',
            'orderField': 'order_create_time',
            'isVoided': '-1',
            'isRemoved': '-1',
            'ruleId': '-1',
            'sysRule': '',
            'applyType': '',
            'applyStatus': '',
            'printJh': '-1',
            'printMd': '-1',
            'commitPlatform': '',
            'productStatus': '',
            'jhComment': '-1',
            'storageId': '0',
            'isOversea': '-1',
            'isFree': '-1',
            'isBatch': '-1',
            'history': '',
            'custom': '-1',
            'timeOut': '0',
            'refundStatus': '0',
            'buyerAccount': '',
            'forbiddenStatus': '-1',
            'forbiddenReason': '0',
            'behindTrack': '-1',
            'orderId': '',
            'axios_cancelToken': 'true'
        }
    
    def fetch_one_page(self, page_no: int) -> Dict:
        try:
            data = self._build_post_data(page_no=page_no, page_size=300)
            
            self._debug_print(f"    [DEBUG] è¯·æ±‚å‚æ•°: pageNo={page_no}, pageSize=300")
            
            response = self.session.post(
                self.base_url,
                headers=self._get_headers(),
                data=data,
                timeout=30
            )
            
            self._debug_print(f"    [DEBUG] HTTPçŠ¶æ€ç : {response.status_code}")
            
            if response.status_code != 200:
                print(f"    âœ— HTTPé”™è¯¯: çŠ¶æ€ç  {response.status_code}")
                self._debug_print(f"    [DEBUG] å“åº”å†…å®¹: {response.text[:500]}")
                return None
            
            try:
                result = response.json()
            except json.JSONDecodeError as e:
                print(f"    âœ— JSONè§£æå¤±è´¥: {str(e)}")
                self._debug_print(f"    [DEBUG] å“åº”å†…å®¹å‰500å­—ç¬¦: {response.text[:500]}")
                return None
            
            self._debug_print(f"    [DEBUG] å“åº”é”®: {list(result.keys())}")
            
            # æ£€æŸ¥codeå­—æ®µè€Œä¸æ˜¯successå­—æ®µ
            if result.get('code') != 0:
                print(f"    âœ— APIè¿”å›å¤±è´¥!")
                self._debug_print(f"    [DEBUG] codeå­—æ®µ: {result.get('code')}")
                self._debug_print(f"    [DEBUG] msgå­—æ®µ: {result.get('msg', 'æ— ')}")
                return None
            
            # æ£€æŸ¥dataå­—æ®µ
            if 'data' not in result:
                print(f"    âœ— å“åº”ä¸­ç¼ºå°‘dataå­—æ®µ")
                return None
            
            data_obj = result.get('data', {})
            
            # æ•°æ®åœ¨ data.page.list ä¸­
            page_obj = data_obj.get('page', {})
            orders = page_obj.get('list', [])
            
            self._debug_print(f"    [DEBUG] è®¢å•æ•°é‡: {len(orders)}")
            
            return result
            
        except requests.exceptions.Timeout as e:
            print(f"    âœ— è¯·æ±‚è¶…æ—¶: {str(e)}")
            return None
        except requests.exceptions.ConnectionError as e:
            print(f"    âœ— è¿æ¥é”™è¯¯: {str(e)}")
            return None
        except Exception as e:
            print(f"    âœ— æœªçŸ¥é”™è¯¯: {type(e).__name__} - {str(e)}")
            return None


def run_scraper(cookie_json_path: str, days: int) -> List[Dict]:
    all_responses = []
    
    print(f"\n[INFO] åˆå§‹åŒ–çˆ¬è™«...")
    print(f"[INFO] Cookieæ–‡ä»¶: {cookie_json_path}")
    print(f"[INFO] æŠ“å–å¤©æ•°: {days}")
    
    try:
        scraper = DianxiaomiScraper(cookie_json_path)
    except Exception as e:
        print(f"\nâœ— åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        return []
    
    today = datetime.now()
    cutoff_date = today - timedelta(days=days)
    
    print(f"\nå¼€å§‹ç¿»é¡µæŠ“å–è®¢å•æ•°æ®...")
    print(f"å½“å‰æ—¥æœŸ: {today.strftime('%Y-%m-%d')}")
    print(f"æˆªæ­¢æ—¥æœŸ: {cutoff_date.strftime('%Y-%m-%d')} (ä¸åŒ…å«)")
    print("=" * 60)
    
    page_no = 1
    total_orders = 0
    in_range_orders = 0
    
    while True:
        print(f"\n  â†’ ç¬¬ {page_no} é¡µ...")
        
        result = scraper.fetch_one_page(page_no)
        
        if not result:
            print(f"    âš ï¸ ç¬¬ {page_no} é¡µè¯·æ±‚å¤±è´¥ï¼Œåœæ­¢æŠ“å–")
            break
        
        all_responses.append(result)
        
        # ä» data.page.list è·å–è®¢å•
        data = result.get('data', {})
        page_obj = data.get('page', {})
        orders = page_obj.get('list', [])
        
        if not orders:
            print(f"    âœ“ æ— æ›´å¤šæ•°æ®")
            break
        
        page_in_range = 0
        page_out_range = 0
        earliest_date = None
        latest_date = None
        
        for order in orders:
            # ä½¿ç”¨orderCreateTimeå­—æ®µï¼ˆæ—¶é—´æˆ³ï¼Œæ¯«ç§’ï¼‰
            order_time_ms = order.get('orderCreateTime', 0)
            if order_time_ms:
                try:
                    order_time = datetime.fromtimestamp(order_time_ms / 1000.0)
                    
                    if earliest_date is None or order_time < earliest_date:
                        earliest_date = order_time
                    if latest_date is None or order_time > latest_date:
                        latest_date = order_time
                    
                    if order_time >= cutoff_date:
                        page_in_range += 1
                    else:
                        page_out_range += 1
                        
                except Exception as e:
                    print(f"    âš ï¸ æ—¥æœŸè§£æå¤±è´¥: {order_time_ms}")
        
        total_orders += len(orders)
        in_range_orders += page_in_range
        
        date_range_str = ""
        if earliest_date and latest_date:
            if earliest_date.date() == latest_date.date():
                date_range_str = f" | æ—¥æœŸ: {earliest_date.strftime('%Y-%m-%d')}"
            else:
                date_range_str = f" | æ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')} ~ {earliest_date.strftime('%Y-%m-%d')}"
        
        print(f"    âœ“ è·å– {len(orders)} æ¡è®¢å• (èŒƒå›´å†…: {page_in_range}, èŒƒå›´å¤–: {page_out_range}){date_range_str}")
        print(f"    [ç´¯è®¡] æ€»è®¢å•: {total_orders}, èŒƒå›´å†…: {in_range_orders}, å“åº”é¡µæ•°: {len(all_responses)}")
        
        if page_out_range > 0 and page_in_range == 0:
            print(f"    âœ“ å·²è¶…å‡ºæ—¥æœŸèŒƒå›´ï¼Œåœæ­¢ç¿»é¡µ")
            break
        
        page_no += 1
        
        delay = random.uniform(3, 5)
        print(f"  â±ï¸  ç­‰å¾… {delay:.1f} ç§’...", end=" ")
        time.sleep(delay)
        print("ç»§ç»­")
    
    print("\n" + "=" * 60)
    print(f"ğŸ‰ æŠ“å–å®Œæˆ!")
    print(f"  - å…±è¯·æ±‚: {len(all_responses)} é¡µ")
    print(f"  - æ€»è®¢å•: {total_orders} æ¡")
    print(f"  - èŒƒå›´å†…: {in_range_orders} æ¡")
    
    return all_responses


if __name__ == "__main__":
    responses = run_scraper(
        cookie_json_path="cookie.json",
        days=3
    )
    
    print(f"\nè¿”å›çš„å“åº”åˆ—è¡¨é•¿åº¦: {len(responses)}")